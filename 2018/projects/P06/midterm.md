---
layout: page
mathjax: true
permalink: /2018/projects/p06/midterm/
---

## 项目进展报告

### 数据获取及预处理
#### 1. 数据集描述
本任务使用数据来自于2016年CCF大数据竞赛搜狗用户画像提供的搜狗搜索数据。该数据集宫包含训练数据20000条、测试数据20000条。  
数据各个字段描述如下：  

**ID**	加密后的ID  
**Age**	0：未知年龄; 1：0-18岁; 2：19-23岁; 3：24-30岁; 4：31-40岁; 5：41-50岁; 6： 51-999岁  
**Gender**	0：未知;1：男性;2：女性  
**Education**	0：未知; 1：博士; 2：硕士; 3：大学生; 4：高中; 5：初中; 6：小学  
**Query List**	搜索词列表  

本任务利用数据集中提供的用户历史一个月的查询词与用户的人口属性标签（包括性别、年龄、学历）做为训练数据，构建数据挖掘模型对新增用户的人口属性进行判定。即对训练数据中的每条记录进行年龄、性别、学历的判断。

#### 2. 数据预处理
##### 对于query文本：
针对于每个用户的query文本，我们进行了去停用词、提取非中文字符的处理，因为部分query中含有的英文单词和数字也可能对分类结果产生影响，因此我们为每个用户数据样本构建一个非中文字符词表。此外，我们将query中含有的网页链接使用“HTML”字符串进行标记替代。  
##### 对于类别标签：
我们剔除了全部有标签缺失的数据样本，剩余训练样本规模为17663条记录。

### 数据分析与可视化

经过对数据集的分析，我们可以得到在训练数据上，性别、年龄、学历的分布如下：  
性别分布：  
<img height = 320 width = 480 src="https://github.com/zhaohe1995/BIT2018-DataMiningHomework/blob/master/Project/images/gender_dist.png?raw=true"/>  
年龄分布：  
<img height = 320 width = 480 src="https://github.com/zhaohe1995/BIT2018-DataMiningHomework/blob/master/Project/images/age_dist.png?raw=true"/>  
学历分布：  
<img height = 320 width = 480 src="https://github.com/zhaohe1995/BIT2018-DataMiningHomework/blob/master/Project/images/edu_dist.png?raw=true"/>  

从分布情况中我们可以发现：  
1.存在部分数据缺失  
2.年龄分布上由1-6依次减少，说明年轻的用户群体占比较大  
3.学历分布上1、2类别较少，多数集中在3、4、5类，且缺失数据较多  
4.存在较严重的数据不平衡问题  

此外，通过对query文本情况的统计，我们可以得到：  
query条数最大值：747  
query条数最小值：50  
query长度最大值：338  
query长度最小值：1  

query条数分布情况：  
<img height = 320 width = 480 src="https://github.com/zhaohe1995/BIT2018-DataMiningHomework/blob/master/Project/images/query_num.png?raw=true"/>  

query长度分布情况：  
<img height = 320 width = 480 src="https://github.com/zhaohe1995/BIT2018-DataMiningHomework/blob/master/Project/images/query_lenth.png?raw=true"/>  
根据分布，我们剔除掉长度大于60的query文本。

### 模型选取

#### 1. 对query进行分词
首先，对于经过预处理的每天用户query文本，我们使用结巴分词工具对短文本进行分词。

#### 2. 基于TF-IDF进行文本特征提取
对于每一个用户，我们将其全部query短文本拼接构成一个用户query文档。然后在用户文档语料上使用TF-IDF构建特征词表（word-bag），并分别对于三个目标分类标签，使用ch2卡方检验进行特征选择，最终构建的特征词表规模为3000，因此对于每个用户文档，我们能够得到一个对应的特征向量。

#### 3. 基于Word2Vec进行本文向量表示
我们使用Mikolov提出的Skip-gram模型（word2vec）在中文维基百科上训练得到一个100维的word embeddings。对于每个用户文档，我们采用对其中每个词的词向量加和取平均的方法得到该用户文档对应的word2vec文档向量（100维）。
#### 4. 使用卷积神经网络（CNN）构建分类器
将特征选择得到的用户文档向量与word2vec得到的用户文档向量进行拼接，作为卷积神经网络的输入。我们构建的CNN由1个卷积层、2个全连接层以及softmax输出层构成。其中卷积核大小为1，2，3，每种大小对应选择8个，共24个卷积核。全连接层为256维、128维。激活函数采用ReLU函数。采用相同的网络结构分别针对三个目标分类标签建立三个模型。

### 挖掘实验的结果

我们初步得到的分类结果如下：

Item | precision | recall | F1
---|---|---|---
性别 | 0.57 | 0.51  | 0.54 |
年龄 | 0.64 | 0.54  | 0.59 |
学历 | 0.43 | 0.49  | 0.46 |


### 存在的问题

##### 问题一：数据样本分布不平衡问题
由于在训练数据集中，年龄分布、学历分布都存在严重的不平衡问题，这使得对分类器的训练造成严重的影响，训练得到的分类模型会倾向于将样本分向频率高的类别。

##### 问题二：标签缺失数据的处理问题
在现阶段的方法中，对于分类标签缺失的数据样本，我们采用的是直接剔除的处理方法。但是这种方法过于简单，会一定程度减小训练集的规模，对分类模型的训练造成影响。

##### 问题三：神经网络结构的设计问题
在我们当前的模型中，将TF-IDF文本特征词向量表示的结果与Word2Vec文本向量的结果进行了简单拼接，直接作为CNN的输入。由于TF-IDF文本特征词向量为3000维且较稀疏，而Word2Vec文本向量我100维的紧密向量，简单的拼接效果可能并不好。


### 下一步工作

##### 针对上述问题一：
我们拟尝试上采样/下采样的方法，使训练数据集中各个分类的分布上保持均匀，再尝试训练分类模型并对比效果。

##### 针对上述问题二：
我们拟尝试采用以下策略对缺失数据进行填补：用训练好的模型对缺失数据的分类标签进行预测，将预测出的标签结果填补到缺失处，再利用经过填补的完整数据集重新训练分类模型。

##### 针对以上问题三：
我们拟尝试在网络结构中增加一层，既先将TF-IDF文本特征词向量映射至低维、紧密向量，再将此向量与Word2Vec词向量进行拼接作为CNN的输入。  

此外，我们还将在后续的工作中尝试模型集成的方法，如boosting、stacking方法等。并选取其他几种机器学习分类器，如SVM、Xgboost等，最后对各个分类器的分类结果进行集成，并对比各种方法得到的结果。
